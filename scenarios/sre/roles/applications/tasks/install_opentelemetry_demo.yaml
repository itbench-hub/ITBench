---
# Note: The Prometheus offered by OpenShift is v2, while the one installed
#       in the chart is v3. While otel-demo supports the `otlphttp` exporter,
#       this is not easily exposed in OpenShift. So, IT-Bench uses the `prometheus`
#       exporter instead to support both platforms until the functionality of
#       Prometheus is the same on both platforms.

# Note: Due to a Helm bug, the value of the `otlphttp` and `opensearch` exports cannot
#       be set to null in order to remove the chart's values. So, the configuration of
#       the exporters is still correctly set, but not used for forwarding the data.
#       Requires backport: https://github.com/helm/helm/issues/30587

- name: Create Namespace
  kubernetes.core.k8s:
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    template: templates/kubernetes/otel_demo/namespace.j2
    state: present

- name: Import tools role for variable setting tasks
  ansible.builtin.import_role:
    name: tools
    tasks_from: set_clickhouse_credentials
  vars:
    tools_cluster:
      kubeconfig: "{{ applications_cluster.kubeconfig }}"

- name: Import tools role for variable setting tasks
  ansible.builtin.import_role:
    name: tools
    tasks_from: set_clickhouse_endpoint
  vars:
    tools_cluster:
      kubeconfig: "{{ applications_cluster.kubeconfig }}"

- name: Import tools role for variable setting tasks
  ansible.builtin.import_role:
    name: tools
    tasks_from: set_jaeger_endpoint
  vars:
    tools_cluster:
      kubeconfig: "{{ applications_cluster.kubeconfig }}"

- name: Import tools role for variable setting tasks
  ansible.builtin.import_role:
    name: tools
    tasks_from: set_prometheus_endpoint
  vars:
    tools_cluster:
      kubeconfig: "{{ applications_cluster.kubeconfig }}"
      platform: "{{ applications_cluster.platform }}"

- name: Install OpenTelemetry Demo (Astronomy Shop)
  kubernetes.core.helm:
    chart_ref: "{{ applications_managers.opentelemetry_demo.helm.chart.reference }}"
    chart_version: "{{ applications_managers.opentelemetry_demo.helm.chart.version }}"
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    release_name: "{{ applications_managers.opentelemetry_demo.helm.release.name }}"
    release_namespace: "{{ applications_managers.opentelemetry_demo.kubernetes.namespace }}"
    release_state: present
    timeout: 10m0s
    values: |-
      {{
        lookup('ansible.builtin.template', 'templates/helm/otel_demo/values.j2') |
        ansible.builtin.from_yaml
      }}
    wait: true

- name: Retrieve all pods
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Pod
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    namespace: "{{ applications_managers.opentelemetry_demo.kubernetes.namespace }}"
  register: applications_otel_demo_pods

- name: Wait for pods to reach ready state
  kubernetes.core.k8s_info:
    api_version: "{{ pod.apiVersion }}"
    kind: "{{ pod.kind }}"
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    name: "{{ pod.metadata.name }}"
    namespace: "{{ pod.metadata.namespace }}"
    wait: true
    wait_condition:
      type: Ready
      status: "True"
    wait_timeout: 300
  loop: "{{ applications_otel_demo_pods.resources }}"
  loop_control:
    label: "pod/{{ pod.metadata.name }}"
    loop_var: pod

- name: Retrieve all deployments
  kubernetes.core.k8s_info:
    api_version: apps/v1
    kind: Deployment
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    namespace: "{{ applications_managers.opentelemetry_demo.kubernetes.namespace }}"
  register: applications_otel_demo_deployments

- name: Create HorizontalPodAutorscalers
  kubernetes.core.k8s:
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    resource_definition:
      apiVersion: autoscaling/v2
      kind: HorizontalPodAutoscaler
      metadata:
        name: "{{ workload.metadata.name }}"
        namespace: "{{ workload.metadata.namespace }}"
      spec:
        scaleTargetRef:
          apiVersion: "{{ workload.apiVersion }}"
          kind: "{{ workload.kind }}"
          name: "{{ workload.metadata.name }}"
        minReplicas: 1
        maxReplicas: 10
        metrics:
          - type: Resource
            resource:
              name: cpu
              target:
                type: Utilization
                averageUtilization: 70
          - type: Resource
            resource:
              name: memory
              target:
                type: Utilization
                averageUtilization: 80
        behavior:
          scaleDown:
            policies:
              - type: Percent
                value: 50
                periodSeconds: 60
              - type: Pods
                value: 2
                periodSeconds: 60
            selectPolicy: Min
    state: present
  loop: |-
      {{
        applications_otel_demo_deployments.resources |
        ansible.builtin.rejectattr('metadata.name', '==', 'load-generator') |
        ansible.builtin.rejectattr('metadata.name', '==', 'opentelemetry-collector')
      }}
  loop_control:
    label: deployment/{{ workload.metadata.name }}
    loop_var: workload
  when:
    - applications_configuration.opentelemetryDemo.options.autoscaling.kubernetes.hpa | ansible.builtin.default(false)

- name: Create Common PrometheusRules
  kubernetes.core.k8s:
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    template:
      path: templates/kubernetes/common/prometheusrules.j2
      variable_end_string: "]]"
      variable_start_string: "[["
    state: present
  vars:
    application_namespace: "{{ applications_managers.opentelemetry_demo.kubernetes.namespace }}"
    group_name: opentelemetrydemo

- name: Create OpenTelemetry Demo PrometheusRules
  kubernetes.core.k8s:
    kubeconfig: "{{ applications_cluster.kubeconfig }}"
    template:
      path: templates/kubernetes/otel_demo/prometheusrules.j2
      variable_end_string: "]]"
      variable_start_string: "[["
    state: present
