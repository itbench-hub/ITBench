---
- name: Retrieve workload
  kubernetes.core.k8s_info:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    api_version: "{{ injection_task.args.kubernetesObject.apiVersion }}"
    kind: "{{ injection_task.args.kubernetesObject.kind }}"
    name: "{{ injection_task.args.kubernetesObject.metadata.name }}"
    namespace: "{{ injection_task.args.kubernetesObject.metadata.namespace }}"
  register: faults_workload

- name: Validate that workload exists
  ansible.builtin.assert:
    that:
      - faults_workload.api_found
      - faults_workload.resources | ansible.builtin.length == 1
    fail_msg: Unable to find workload. Please verify that the workload exists.
    success_msg: Found workload.

# TODO: We need to add node labels to specific worker nodes so that the
#       the tool stack is not impacted by these faults.

- name: Retrieve worker nodes
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Node
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    label_selectors:
      - "!node-role.kubernetes.io/control-plane"
  register: faults_worker_nodes

- name: Validate that worker nodes exists
  ansible.builtin.assert:
    that:
      - faults_worker_nodes.api_found
      - faults_worker_nodes.resources | ansible.builtin.length > 0
    fail_msg: Unable to find worker nodes. Please run this fault on a cluster with worker nodes.
    success_msg: Found worker nodes.

- name: Select an available worker node randomly
  ansible.builtin.set_fact:
    faults_node: "{{ faults_worker_nodes.resources | ansible.builtin.random }}"

- name: Create PriorityClass (high priority)
  kubernetes.core.k8s:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    resource_definition:
      apiVersion: scheduling.k8s.io/v1
      kind: PriorityClass
      metadata:
        labels:
          app.kubernetes.io/managed-by: ITBench
        name: application-critical
      value: 10000
      globalDefault: false
      description: High priorty class for application workloads
    state: present

- name: Create PriorityClass (lower priority)
  kubernetes.core.k8s:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    resource_definition:
      apiVersion: scheduling.k8s.io/v1
      kind: PriorityClass
      metadata:
        labels:
          app.kubernetes.io/managed-by: ITBench
        name: application-non-critical
      value: 1
      globalDefault: false
      description: Lower priorty class for application workloads
    state: present

- name: Patch workload with priority class and assign to node
  kubernetes.core.k8s:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    resource_definition:
      apiVersion: "{{ faults_workload.resources[0].apiVersion }}"
      kind: "{{ faults_workload.resources[0].kind }}"
      metadata:
        name: "{{ faults_workload.resources[0].metadata.name }}"
        namespace: "{{ faults_workload.resources[0].metadata.namespace }}"
      spec:
        template:
          spec:
            nodeSelector:
              "kubernetes.io/hostname": "{{ faults_node.metadata.name }}"
            priorityClassName: application-non-critical
    state: patched

- name: Wait for workload to update
  kubernetes.core.k8s_info:
    api_version: "{{ faults_workload.resources[0].apiVersion }}"
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    kind: "{{ faults_workload.resources[0].kind }}"
    name: "{{ faults_workload.resources[0].metadata.name }}"
    namespace: "{{ faults_workload.resources[0].metadata.namespace }}"
  register: faults_patched_workload
  until:
    - faults_patched_workload.api_found
    - faults_patched_workload.resources | ansible.builtin.length == 1
    - faults_patched_workload.resources[0].status.availableReplicas == faults_patched_workload.resources[0].status.replicas
  delay: 15
  retries: 20

- name: Create priority workload
  kubernetes.core.k8s:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    resource_definition:
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          app.kubernetes.io/name: critical-data-collector
        name: critical-data-collector
        namespace: "{{ faults_workload.resources[0].metadata.namespace }}"
      spec:
        replicas: "{{ replicas + 1 }}"
        selector:
          matchLabels:
            app.kubernetes.io/name: critical-data-collector
        template:
          metadata:
            labels:
              app.kubernetes.io/name: critical-data-collector
          spec:
            containers:
              - name: worker
                # renovate: datasource=docker depName=registry.access.redhat.com/ubi10-minimal
                image: registry.access.redhat.com/ubi10-minimal:10.1-1770180557
                command:
                  - /bin/sh
                args:
                  - -c
                  - "sleep infinity"
                resources:
                  requests:
                    cpu: "{{ ((((faults_node.status.allocatable.cpu | int) - 1) / replicas) * 1000) }}m"
                    memory: 1Mi
            nodeSelector:
              "kubernetes.io/hostname": "{{ faults_node.metadata.name }}"
            priorityClassName: application-critical
    state: present
    wait: true
  vars:
    replicas: 20
