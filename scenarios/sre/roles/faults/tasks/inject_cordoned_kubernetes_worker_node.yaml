---
- name: Retrieve workload
  kubernetes.core.k8s_info:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    api_version: "{{ injection_task.args.kubernetesObject.apiVersion }}"
    kind: "{{ injection_task.args.kubernetesObject.kind }}"
    name: "{{ injection_task.args.kubernetesObject.metadata.name }}"
    namespace: "{{ injection_task.args.kubernetesObject.metadata.namespace }}"
  register: faults_workload

- name: Validate that workload exists
  ansible.builtin.assert:
    that:
      - faults_workload is defined
      - faults_workload.resources | ansible.builtin.length == 1
    fail_msg: Unable to find workload. Please verify that the workload exists.
    success_msg: Found workload.

# TODO: We need to add node labels to specific worker nodes so that the
#       the tool stack is not impacted by these faults.

- name: Retrieve worker nodes
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Node
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    label_selectors:
      - node-role.kubernetes.io/node =
  register: faults_worker_nodes

- name: Validate that worker nodes exists
  ansible.builtin.assert:
    that:
      - faults_worker_nodes is defined
      - faults_worker_nodes.resources | ansible.builtin.length > 0
    fail_msg: Unable to find worker nodes. Please run this fault on a cluster with worker nodes.
    success_msg: Found worker nodes.

- name: Select an available worker node randomly
  ansible.builtin.set_fact:
    faults_node: "{{ faults_worker_nodes.resources | ansible.builtin.random }}"

- name: Assign workload to selected worker node
  kubernetes.core.k8s:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    resource_definition:
      apiVersion: "{{ faults_workload.resources[0].apiVersion }}"
      kind: "{{ faults_workload.resources[0].kind }}"
      metadata:
        name: "{{ faults_workload.resources[0].metadata.name }}"
        namespace: "{{ faults_workload.resources[0].metadata.namespace }}"
      spec:
        template:
          spec:
            nodeSelector:
              "kubernetes.io/hostname": "{{ faults_node.metadata.name }}"
    state: patched

- name: Wait for workload to update
  kubernetes.core.k8s_info:
    api_version: "{{ faults_workload.resources[0].apiVersion }}"
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    kind: "{{ faults_workload.resources[0].kind }}"
    name: "{{ faults_workload.resources[0].metadata.name }}"
    namespace: "{{ faults_workload.resources[0].metadata.namespace }}"
  register: faults_patched_workload
  until:
    - faults_patched_workload.resources | ansible.builtin.length == 1
    - faults_patched_workload.resources[0].status.availableReplicas == faults_patched_workload.resources[0].status.replicas
  delay: 15
  retries: 20

- name: Cordon the selected worker node
  kubernetes.core.k8s_drain:
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    name: "{{ faults_node.metadata.name }}"
    state: cordon

# NOTE: Do not wait for the following operation to complete successfully.
#       It will not because the node is cordoned and thus no new pods can
#       be created on the node.

- name: Scale workload
  kubernetes.core.k8s_scale:
    api_version: "{{ faults_workload.resources[0].apiVersion }}"
    kubeconfig: "{{ faults_cluster.kubeconfig }}"
    kind: "{{ faults_workload.resources[0].kind }}"
    name: "{{ faults_workload.resources[0].metadata.name }}"
    namespace: "{{ faults_workload.resources[0].metadata.namespace }}"
    replicas: "{{ faults_workload.resources[0].spec.replicas + 1 }}"
    wait: false
